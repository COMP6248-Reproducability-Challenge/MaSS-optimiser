import numpy as np
import tensorflow as tf
import keras
from tensorflow.keras.optimizers import Adam, SGD
#from keras.optimizers import Adam, SGD
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from optimizers import MaSS
from cifar10 import load
from resnet import resnet_v1, resnet_v2
import os


batch_size = 64  
epochs = 200
data_augmentation = True
num_classes = 10

# Load Cifar-10 data
(x_train, y_train), (x_test, y_test) = load()
input_shape = x_train.shape[1:]

# Model parameters
n = 5
version = 1
if version == 1:
    depth = n * 6 + 2
elif version == 2:
    depth = n * 9 + 2

# Model name, depth and version
model_type = 'ResNet%dv%d' % (depth, version)


################################################################
# Whenever learning rate reduces, restart the MaSS optimizer at the latest learned weights.
# (If not reducing learning rate during training, one can have only one stage.)
#
#Stage 1: epoch 1-150. Learning rate: 0.1
################################################################

# Build model
model = resnet_v1(input_shape=input_shape, depth=depth)
mass = MaSS(lr = 0.1, alpha = 0.05, kappa_t = 2)
model.compile(loss='categorical_crossentropy',
              optimizer=mass,
              metrics=['accuracy'])
model.summary()

# Prepare model saving directory.
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_name = 'cifar10_%s_model_mass.{epoch:03d}.h5' % model_type
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
filepath = os.path.join(save_dir, model_name)
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=False)
callbacks = [checkpoint]

# Training
if not data_augmentation:
    result1 = model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=100,
              validation_data=(x_test, y_test),
              shuffle=True,
              callbacks=callbacks)
else:
    print('Using real-time data augmentation.')

    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,           	# set input mean to 0 over the dataset
        samplewise_center=False,            	# set each sample mean to 0        
        featurewise_std_normalization=False,    # divide inputs by std of dataset        
        samplewise_std_normalization=False,     # divide each input by its std
        zca_whitening=False,                    # apply ZCA whitening        
        rotation_range=0,                       # randomly rotate images in the range (deg 0 to 180)        
        width_shift_range=0.1,                  # randomly shift images horizontally        
        height_shift_range=0.1,                 # randomly shift images vertically        
        horizontal_flip=True,                   # randomly flip images        
        vertical_flip=False)                    # randomly flip images

    res1 = datagen.fit(x_train)
    # Fit the model on the batches generated by datagen.flow().
    result1 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                        validation_data=(x_test, y_test),
                        epochs=150, verbose=1, workers=4,
                                 callbacks=callbacks)


################################################################
# Stage 2: epoch 151-225. Learning rate: 0.01 (i.e. lr reduce by 10 after 150 epochs)
################################################################

# Build model
model = resnet_v1(input_shape=input_shape, depth=depth)
mass = MaSS(lr = 0.01, alpha = 0.05, kappa_t = 2)
model.compile(loss='categorical_crossentropy',
              optimizer=mass,
              metrics=['accuracy'])
model.summary()

# Load weights
model.load_weights('saved_models/cifar10_ResNet32v1_model_mass.150.h5')

# Prepare model saving directory.
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_name = 'cifar10_%s_model_mass_150+.{epoch:03d}.h5' % model_type
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
filepath = os.path.join(save_dir, model_name)
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=False)
callbacks = [checkpoint]

# Training
if not data_augmentation:
    result2 = model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=100,
              validation_data=(x_test, y_test),
              shuffle=True,
              callbacks=callbacks)
else:
    print('Using real-time data augmentation.')
    # Fit the model on the batches generated by datagen.flow().
    result2 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                        validation_data=(x_test, y_test),
                        epochs=75, verbose=1, workers=4,
                                 callbacks=callbacks)

################################################################
# Stage 3: epoch 226-300. Learning rate: 0.001 (i.e. lr reduce by 10 after 225 epochs)
################################################################

# Build model
model = resnet_v1(input_shape=input_shape, depth=depth)
mass = MaSS(lr = 0.001, alpha = 0.05, kappa_t = 2)
model.compile(loss='categorical_crossentropy',
              optimizer=mass,
              metrics=['accuracy'])

# Load weights
model.load_weights('saved_models/cifar10_ResNet32v1_model_mass_150+.075.h5')

# Prepare model saving directory.
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_name = 'cifar10_%s_model_mass_225+.{epoch:03d}.h5' % model_type
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
filepath = os.path.join(save_dir, model_name)
checkpoint = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=False)
callbacks = [checkpoint]

# Training
if not data_augmentation:
    result3 = model.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=100,
              validation_data=(x_test, y_test),
              shuffle=True,
              callbacks=callbacks)
else:
    print('Using real-time data augmentation.')

    # Fit the model on the batches generated by datagen.flow().
    result3 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                        validation_data=(x_test, y_test),
                        epochs=75, verbose=1, workers=4,
                                 callbacks=callbacks)




# Score trained model.
scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])


